{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1G2uVyML_6kphmce26V_RV6EPBmTJC-ts",
      "authorship_tag": "ABX9TyOu+/5HdYW4mX1GpY7QkCht",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neilus03/Restb.ai_challenge-HackUPC2023/blob/Neil_branch/TABNET2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_tabnet"
      ],
      "metadata": {
        "id": "tSHIgMd-ldwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpIqD1BxkVYw"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load data\n",
        "Big_Data = pd.read_pickle(\"/content/drive/MyDrive/HackUPC/HackUPC_Restb.ia/hackupc2023_restbai.pkl\")\n",
        "Big_Data = Big_Data.T\n"
      ],
      "metadata": {
        "id": "iQiOGk_IlZzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Data preprocessing\n",
        "city2number = {key: indice for indice, key in enumerate(Big_Data[\"city\"].unique())}\n",
        "neighborhood2number = {key: indice for indice, key in enumerate(Big_Data[\"neighborhood\"].unique())}\n",
        "region2number = {key: indice for indice, key in enumerate(Big_Data[\"region\"].unique())}\n",
        "\n",
        "Big_Data['city'] = Big_Data['city'].map(city2number)\n",
        "Big_Data['neighborhood'] = Big_Data['neighborhood'].map(neighborhood2number)\n",
        "Big_Data['region'] = Big_Data['region'].map(region2number)\n",
        "\n",
        "Big_Data = Big_Data.drop(columns = [\"summary\", \"images\", \"image_data\", \"property_type\", \"num_images\"])\n",
        "Big_Data = Big_Data.fillna(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "MuukpLFzkZfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split data\n",
        "train_data, valid_data = train_test_split(Big_Data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize data\n",
        "train_mean = train_data.mean()\n",
        "train_std = train_data.std()\n",
        "\n",
        "train_data = (train_data - train_mean) / train_std\n",
        "valid_data = (valid_data - train_mean) / train_std\n",
        "\n",
        "# Prepare data for TabNet\n",
        "features = [col for col in train_data.columns if col != 'price']\n",
        "target = ['price']\n",
        "\n",
        "X_train = train_data[features].values\n",
        "y_train = train_data[target].values.reshape(-1, 1)\n",
        "X_valid = valid_data[features].values\n",
        "y_valid = valid_data[target].values.reshape(-1, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "VvmS-75jkcM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize TabNetRegressor\n",
        "model = TabNetRegressor()\n",
        "\n",
        "# Fit model\n",
        "model.fit(\n",
        "    X_train=X_train, y_train=y_train,\n",
        "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "    eval_name=['train', 'valid'],\n",
        "    eval_metric=['rmse'],\n",
        "    max_epochs=100,\n",
        "    patience=10,\n",
        "    batch_size=256, \n",
        "    virtual_batch_size=128,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YkbnzVSLkhcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on validation set\n",
        "y_pred = model.predict(X_valid)"
      ],
      "metadata": {
        "id": "GycSKF6_kkVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute RMSE\n",
        "rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
        "print(f'Validation RMSE: {rmse}')\n"
      ],
      "metadata": {
        "id": "yDiL_GhbkhNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Let's take a specific instance from the validation set to see the prediction\n",
        "instance_index = 10\n",
        "single_instance = X_valid[instance_index]\n",
        "single_true_target = y_valid[instance_index]\n",
        "\n",
        "# Making prediction\n",
        "single_pred_target = model"
      ],
      "metadata": {
        "id": "48qMCi2rklx0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}